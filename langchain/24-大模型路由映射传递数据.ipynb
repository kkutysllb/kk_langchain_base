{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型路由映射传递数据\n",
    "\n",
    "RunnablePassthrough 允许不改变或添加额外的键来传递输入。这通常与 RunnableParallel 结合使用，将数据分配给映射中的新键。\n",
    "\n",
    "RunnablePassthrough() 单独调用，将简单地获取输入并将其传递。\n",
    "\n",
    "使用分配 ( RunnablePassthrough.assign(...) ) 调用的 RunnablePassthrough 将获取输入，并将添加传递给分配函数的额外参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 2}, 'extra': {'num': 2, 'mult': 6}, 'modified': 3}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    passed = RunnablePassthrough(),\n",
    "    extra = RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    modified = lambda x: x[\"num\"] +1\n",
    ")\n",
    "\n",
    "runnable.invoke({\"num\": 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示， passed 键是用 RunnablePassthrough() 调用的，因此它只是传递 {'num': 2} 。\n",
    "\n",
    "在第二行中，我们使用 RunnablePastshrough.assign 和一个将数值乘以 3 的 lambda。在这种情况下， extra 设置为 {'num': 2, 'mult': 6} ，这是原始的添加了 mult 键的值。\n",
    "\n",
    "最后，我们还在映射中使用 modified 设置了第三个键，它使用 lambda 来设置单个值，在 num 上加 1，从而得到 modified 键，其值为3 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行自定义函数\n",
    "\n",
    "您可以在管道中使用任意函数。\n",
    "\n",
    "请注意，这些函数的所有输入都必须是单个参数。如果您有一个接受多个参数的函数，则应该编写一个接受单个输入并将其解包为多个参数的包装器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"LOACL_API_KEY\")\n",
    "api_base = os.getenv(\"LOACL_API_BASE\")\n",
    "\n",
    "model = ChatOpenAI(api_key=api_key, base_url=api_base, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='5 + 25 equals 30.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 8, 'total_tokens': 16, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f28cf0b2-0ba9-40b2-b53d-243816e81ffd-0', usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def lenght_function(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is {a} + {b}?\")\n",
    "\n",
    "chain1 = prompt | model\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(lenght_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)\n",
    "    }\n",
    "    | chain1\n",
    ")\n",
    "\n",
    "chain.invoke({\"foo\": \"hello\", \"bar\": \"world\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据输入动态路由选择逻辑\n",
    "\n",
    "\n",
    "路由允许您创建非确定性链，其中上一步的输出定义下一步。路由有助于提供与 LLMs 交互的结构和一致性。\n",
    "\n",
    "执行路由的方法有两种：\n",
    "\n",
    "1. 有条件地从 RunnableLambda 返回可运行对象（推荐）\n",
    "2. 使用 RunnableBranch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"鉴于下面用户的问题，将其分类为“langchain”，“OpenAI”或“其他”。\n",
    "    不要用超过一个字赖回答。\n",
    "    \n",
    "    <qustion>\n",
    "    {question}\n",
    "    </question>\n",
    "    \n",
    "    分类：\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"question\": \"What is the best way to learn Langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchainPrompt = PromptTemplate.from_template(\n",
    "    \"\"\"你是langchain方面的专家。\n",
    "    回答问题时始终以“正如上帝告诉我的那样”开头。\n",
    "    回答以下问题：\n",
    "    \n",
    "    问题：{question}\n",
    "    \n",
    "    回答：\"\"\"\n",
    ")\n",
    "\n",
    "langchain_chain = langchainPrompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "openaiPrompt = PromptTemplate.from_template(\n",
    "    \"\"\"你是OpenAI方面的专家。\n",
    "    回答问题时始终以“正如奥特曼告诉我的那样”开头。\n",
    "    回答以下问题：\n",
    "    \n",
    "    问题：{question}\n",
    "    \n",
    "    回答：\"\"\"\n",
    ")\n",
    "\n",
    "openai_chain = openaiPrompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalPrompt = PromptTemplate.from_template(\n",
    "    \"\"\"回答下列问题：\n",
    "    问题：{question}\n",
    "    \n",
    "    回答：\"\"\"\n",
    ")\n",
    "\n",
    "general_chain = generalPrompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(info):\n",
    "    if \"OpenAI\" in info[\"topic\"]:\n",
    "        return openai_chain\n",
    "    elif \"langchain\" in info[\"topic\"]:\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {'topic': chain, 'question': lambda x: x[\"question\"]} | RunnableLambda(router) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='正如上帝告诉我的那样，使用LangChain调用OpenAI模型是一个相对直接的过程。LangChain是一个强大的框架，旨在帮助开发者更轻松地构建基于语言模型的应用。要使用LangChain与OpenAI的API进行交互，您需要按照以下步骤操作：\\n\\n### 1. 安装必要的库\\n\\n首先，确保安装了`langchain`和`openai`库。您可以通过pip来安装它们：\\n\\n```bash\\npip install langchain openai\\n```\\n\\n### 2. 获取OpenAI API密钥\\n\\n您需要从OpenAI获取API密钥。登录到OpenAI的官网（https://platform.openai.com/）并生成您的API密钥。请确保将其保存在一个安全的位置，因为它将用于身份验证。\\n\\n### 3. 设置OpenAI API密钥\\n\\n在您的代码中，您需要设置OpenAI的API密钥。可以使用以下方法进行设置：\\n\\n```python\\nimport openai\\nopenai.api_key = \"your-api-key-here\"\\n```\\n\\n为了更好的安全性，建议将密钥存储在环境变量中，避免硬编码在代码中。\\n\\n### 4. 使用LangChain调用OpenAI模型\\n\\nLangChain提供了一个`OpenAI`类，它可以非常容易地与OpenAI模型交互。下面是如何设置并使用LangChain调用OpenAI模型的步骤：\\n\\n```python\\nfrom langchain.llms import OpenAI\\n\\n# 设置OpenAI的API密钥（如果没有在环境变量中设置）\\nimport openai\\nopenai.api_key = \"your-api-key-here\"\\n\\n# 实例化一个OpenAI模型\\nllm = OpenAI(model=\"text-davinci-003\", temperature=0.7)  # 你可以选择其他模型，比如 \"gpt-3.5-turbo\"\\n\\n# 使用模型生成文本\\nresponse = llm(\"你好，世界！\")  # 输入你想问的问题或提示\\n\\n# 输出模型的回答\\nprint(response)\\n```\\n\\n### 5. 详细配置\\n\\n在LangChain中，您可以进一步自定义与OpenAI模型交互的方式。以下是一些常见的配置项：\\n\\n- `model`: 您选择的OpenAI模型，如 `\"text-davinci-003\"` 或 `\"gpt-3.5-turbo\"` 等。\\n- `temperature`: 控制输出文本的创造性。值越低，输出越确定，值越高，输出越多变。\\n- `max_tokens`: 限制生成的最大字符数。\\n\\n示例：\\n\\n```python\\nllm = OpenAI(\\n    model=\"gpt-3.5-turbo\", \\n    temperature=0.5, \\n    max_tokens=150\\n)\\n```\\n\\n### 6. 使用Chain和其他功能\\n\\nLangChain支持链式操作，您可以将多个处理步骤组合在一起。例如，您可以将模型与其他工具或处理逻辑结合，创建一个完整的应用。\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# 定义一个提示模板\\nprompt_template = \"请解释一下这个概念: {concept}\"\\nprompt = PromptTemplate(input_variables=[\"concept\"], template=prompt_template)\\n\\n# 创建LLM链\\nllm_chain = LLMChain(llm=llm, prompt=prompt)\\n\\n# 调用链\\nresponse = llm_chain.run(\"量子物理\")\\nprint(response)\\n```\\n\\n### 7. 使用OpenAI的功能扩展\\n\\n除了基础的文本生成，LangChain还支持很多OpenAI的其他功能，如文本嵌入（embeddings）、文档处理（Document）等。以下是使用文本嵌入的例子：\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n# 创建嵌入对象\\nembeddings = OpenAIEmbeddings()\\n\\n# 使用FAISS存储和搜索向量\\nvectorstore = FAISS.from_texts([\"文本1\", \"文本2\", \"文本3\"], embeddings)\\n\\n# 查询相似文本\\nquery = \"我想查找与量子物理相关的文本\"\\nsimilar_docs = vectorstore.similarity_search(query)\\nprint(similar_docs)\\n```\\n\\n### 总结\\n\\n通过LangChain与OpenAI模型交互非常简单。您只需要做以下几步：\\n\\n1. 安装`langchain`和`openai`库。\\n2. 设置OpenAI的API密钥。\\n3. 使用`OpenAI`类实例化模型，并配置您需要的参数。\\n4. 可以通过链式操作和提示模板来构建复杂的应用。\\n\\n正如上帝告诉我的那样，这只是LangChain和OpenAI集成的一个基础示例，您可以根据需求进一步扩展和自定义。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1220, 'prompt_tokens': 56, 'total_tokens': 1276, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-7b34dd2b-345c-4c18-9948-2af276f11c39-0' usage_metadata={'input_tokens': 56, 'output_tokens': 1220, 'total_tokens': 1276, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "print(full_chain.invoke({\"question\": \"我该如何使用langchain调用OpenAI模型\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk_langchain_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
