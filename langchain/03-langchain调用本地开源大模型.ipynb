{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain调用本地开源大模型\n",
    "\n",
    "1. **类属性定义**:\n",
    "   - `max_token`: 定义了模型可以处理的最大令牌数。\n",
    "   - `do_sample`: 指定是否在生成文本时采用采样策略。\n",
    "   - `temperature`: 控制生成文本的随机性，较高的值会产生更多随机性。\n",
    "   - `top_p`: 一种替代`temperature`的采样策略，这里设置为0.0，意味着不使用。\n",
    "   - `tokenizer`: 分词器，用于将文本转换为模型可以理解的令牌。\n",
    "   - `model`: 存储加载的模型对象。\n",
    "   - `history`: 存储对话历史。\n",
    "2. **构造函数**:\n",
    "   - `__init__`: 构造函数初始化了父类的属性。\n",
    "3. **属性方法**:\n",
    "   - `_llm_type`: 返回模型的类型，即`ChatGLM3`。\n",
    "4. **加载模型的方法**:\n",
    "   - `load_model`: 此方法用于加载模型和分词器。它首先尝试从指定的路径加载分词器，然后加载模型，并将模型设置为评估模式。这里的模型和分词器是从Hugging Face的`transformers`库中加载的。\n",
    "5. **调用方法**:\n",
    "   - `_call`: 一个内部方法，用于调用模型。它被设计为可以被子类覆盖。\n",
    "   - `invoke`: 这个方法使用模型进行聊天。它接受一个提示和一个历史记录，并返回模型的回复和更新后的历史记录。这里使用了模型的方法`chat`来生成回复，并设置了采样、最大长度和温度等参数。\n",
    "6. **流式方法**:\n",
    "   - `stream`: 这个方法允许模型逐步返回回复，而不是一次性返回所有内容。这对于长回复或者需要实时显示回复的场景很有用。它通过模型的方法`stream_chat`实现，并逐块返回回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/libing/miniconda3/envs/kk_agent_base/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class kk_ChatGLM3(LLM):\n",
    "    max_token: int = 8192\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.3\n",
    "    top_p: float = 0.0\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history: list = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"kk_ChatGLM3\"\n",
    "    \n",
    "    def load_model(self, model_path: str):\n",
    "        # 配置分词器\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "        # 加载模型\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
    "        model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "    def _call(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        return self.invoke(prompt, config, history)\n",
    "    \n",
    "    def invoke(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.is_string()\n",
    "        \n",
    "        response, history = self.model.chat(\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            history=history,\n",
    "            do_sample=self.do_sample,\n",
    "            max_length=self.max_token,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        self.history = history\n",
    "        return AIMessage(content=response)\n",
    "    \n",
    "    def stream(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.is_string()\n",
    "        preResponse = \"\"\n",
    "        for response, new_history in self.model.stream_chat(self.tokenizer, prompt):\n",
    "            if preResponse == \"\":\n",
    "                result = response\n",
    "            else:\n",
    "                result = response[len(preResponse):]\n",
    "            preResponse = response\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.88s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "llm = kk_ChatGLM3()\n",
    "model_path = \"/Users/libing/kk_LLMs/llama3-8b-instruct-chinese\"\n",
    "llm.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaModel' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 调用call方法\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m中国的首都是哪里？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mkk_ChatGLM3.invoke\u001b[0;34m(self, prompt, config, history)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     36\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mis_string()\n\u001b[0;32m---> 38\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     40\u001b[0m     prompt,\n\u001b[1;32m     41\u001b[0m     history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m     42\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_sample,\n\u001b[1;32m     43\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token,\n\u001b[1;32m     44\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m history\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AIMessage(content\u001b[38;5;241m=\u001b[39mresponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/kk_agent_base/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaModel' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "# 调用call方法\n",
    "\n",
    "llm.invoke(\"中国的首都是哪里？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk_agent_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
