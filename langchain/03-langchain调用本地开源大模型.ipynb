{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain调用本地开源大模型\n",
    "\n",
    "1. **类属性定义**:\n",
    "   - `max_token`: 定义了模型可以处理的最大令牌数。\n",
    "   - `do_sample`: 指定是否在生成文本时采用采样策略。\n",
    "   - `temperature`: 控制生成文本的随机性，较高的值会产生更多随机性。\n",
    "   - `top_p`: 一种替代`temperature`的采样策略，这里设置为0.0，意味着不使用。\n",
    "   - `tokenizer`: 分词器，用于将文本转换为模型可以理解的令牌。\n",
    "   - `model`: 存储加载的模型对象。\n",
    "   - `history`: 存储对话历史。\n",
    "2. **构造函数**:\n",
    "   - `__init__`: 构造函数初始化了父类的属性。\n",
    "3. **属性方法**:\n",
    "   - `_llm_type`: 返回模型的类型，即`ChatGLM3`。\n",
    "4. **加载模型的方法**:\n",
    "   - `load_model`: 此方法用于加载模型和分词器。它首先尝试从指定的路径加载分词器，然后加载模型，并将模型设置为评估模式。这里的模型和分词器是从Hugging Face的`transformers`库中加载的。\n",
    "5. **调用方法**:\n",
    "   - `_call`: 一个内部方法，用于调用模型。它被设计为可以被子类覆盖。\n",
    "   - `invoke`: 这个方法使用模型进行聊天。它接受一个提示和一个历史记录，并返回模型的回复和更新后的历史记录。这里使用了模型的方法`chat`来生成回复，并设置了采样、最大长度和温度等参数。\n",
    "6. **流式方法**:\n",
    "   - `stream`: 这个方法允许模型逐步返回回复，而不是一次性返回所有内容。这对于长回复或者需要实时显示回复的场景很有用。它通过模型的方法`stream_chat`实现，并逐块返回回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libing/miniconda3/envs/kk_langchain_wsl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/libing/miniconda3/envs/kk_langchain_wsl/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "class kk_ChatGLM3(LLM):\n",
    "    max_token: int = 8192\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.3\n",
    "    top_p: float = 0.0\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history: list = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"kk_ChatGLM3\"\n",
    "    \n",
    "    def load_model(self, model_path: str):\n",
    "        # 配置分词器\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "        # 加载模型\n",
    "        model = AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
    "        model = model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "    def _call(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        return self.invoke(prompt, config, history)\n",
    "    \n",
    "    def invoke(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.is_string()\n",
    "        \n",
    "        response, history = self.model.chat(\n",
    "            self.tokenizer,\n",
    "            prompt,\n",
    "            history=history,\n",
    "            do_sample=self.do_sample,\n",
    "            max_length=self.max_token,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        self.history = history\n",
    "        return AIMessage(content=response)\n",
    "    \n",
    "    def stream(self, prompt: str, config: dict ={}, history: list = []):\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.is_string()\n",
    "        preResponse = \"\"\n",
    "        for response, new_history in self.model.stream_chat(self.tokenizer, prompt):\n",
    "            if preResponse == \"\":\n",
    "                result = response\n",
    "            else:\n",
    "                result = response[len(preResponse):]\n",
    "            preResponse = response\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libing/miniconda3/envs/kk_langchain_wsl/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/libing/miniconda3/envs/kk_langchain_wsl/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "llm = kk_ChatGLM3()\n",
    "model_path = \"/home/libing/kk_LLMs/chatglm3-6b-32k\"\n",
    "llm.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='中国的首都是北京。', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用call方法\n",
    "\n",
    "llm.invoke(\"中国的首都是哪里？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kk_agent_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
